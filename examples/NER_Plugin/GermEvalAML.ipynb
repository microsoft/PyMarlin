{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For automatic reloading of modified libraries\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Regular python libraries\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "import torch\n",
    "\n",
    "# AzureML libraries\n",
    "import azureml\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Datastore, ScriptRunConfig\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.core.runconfig import MpiConfiguration, RunConfiguration, DEFAULT_GPU_IMAGE\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "To create or access an Azure ML Workspace, you will need to import the AML library and the following information:\n",
    "* A name for your workspace\n",
    "* Your subscription id\n",
    "* The resource group name\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step or create a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '<subscription_id>'\n",
    "resource_group = '<resource_group>'\n",
    "workspace_name = '<workspace_name>'\n",
    "ws = Workspace(subscription_id, resource_group, workspace_name)\n",
    "ws_details = ws.get_details()\n",
    "print('Name:\\t\\t{}\\nLocation:\\t{}'\n",
    "      .format(ws_details['name'],\n",
    "              ws_details['location']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "#ds = Datastore.get(ws,'default')\n",
    "#ds = Datastore.get(ws,'workspaceblobstore')\n",
    "print('Datastore name: ' + ds.name, \n",
    "      'Container name: ' + ds.container_name, \n",
    "      'Datastore type: ' + ds.datastore_type, \n",
    "      'Workspace name: ' + ds.workspace.name, sep = '\\n')\n",
    "# ws.get_default_datastore().container_name\n",
    "# ws.datastores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the compute cluster\n",
    "gpu_cluster_name = \"<cluster name>\"\n",
    "\n",
    "# Verify that the cluster doesn't exist already\n",
    "try:\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    if gpu_compute_target.provisioning_state == 'Failed':\n",
    "        gpu_compute_target.delete()\n",
    "        gpu_compute_target.wait_for_completion(show_output=True)\n",
    "        raise ComputeTargetException('failed cluster')\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_priority='lowpriority' , \n",
    "                                                           vm_size='Standard_NC24rs_v3', \n",
    "                                                           min_nodes=0, max_nodes=16)\n",
    "    # ^^^ Change to min_nodes=8 and max_nodes=64 when testing is completed^^^\n",
    "    \n",
    "    # create the cluster\n",
    "    gpu_compute_target = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "    gpu_compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(gpu_compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "\n",
    "run_user_managed = RunConfiguration()\n",
    "run_user_managed.environment.python.user_managed_dependencies = True"
   ]
  },
  {
   "source": [
    "### Germ Eval setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = 'test.py'\n",
    "codepath = '.'\n",
    "\n",
    "from azureml.core import Dataset\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "#create input/output datasets\n",
    "def ds_input_path(path):\n",
    "    return Dataset.File.from_files(ds.path(path))\n",
    "def ds_output_path(path):\n",
    "        return OutputFileDatasetConfig(destination=(ds, path))\n",
    "\n",
    "def get_args():\n",
    "    all_params_default = [\n",
    "                    '--data.train_filepath', './train_germ/train.tsv',\n",
    "                    '--data.val_filepath', './val_germ/dev.tsv',\n",
    "                    '--config_path', 'config_germ.yaml',\n",
    "                    '--ckpt.model_state_save_dir', './mod_ckpts',\n",
    "                    '--ckpt.save_dir', './ckpts'\n",
    "    ]\n",
    "    return all_params_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "myenv = Environment(name=\"myenv\")\n",
    "myenv.docker.base_image = 'jonrsleep/elr2:latest'\n",
    "myenv.python.interpreter_path = '/opt/miniconda/envs/elr2/bin/python'\n",
    "myenv.python.user_managed_dependencies = True\n",
    "\n",
    "mpi = MpiConfiguration() \n",
    "mpi.process_count_per_node = 1 #NC SKU has 4 GPU's per node\n",
    "mpi.node_count = 1 #scale to the amount of nodes you'd like\n",
    "\n",
    "config = ScriptRunConfig(source_directory=codepath,\n",
    "                         script=script_name,\n",
    "                         arguments = get_args(),\n",
    "                         compute_target=gpu_compute_target,\n",
    "                         environment=myenv,\n",
    "                         distributed_job_config=mpi)\n",
    "\n",
    "experiment_name = 'marlin_ner_train_plugin_germ'\n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "run = experiment.submit(config)\n",
    "run.tag('nodes', f'{mpi.node_count}')\n",
    "run.tag('exp', 'lr 3e-5 ')\n",
    "print(\"Submitted run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distrib eval test\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "source": [
    "### Model checkpoint modification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "state_dict = torch.load('marlin_0.bin', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Modify to point to model \n",
    "new_dict = OrderedDict((key.replace('model.',''), value) for key, value in state_dict['module_interface_state'].items() if key.startswith('model.') )\n",
    "#print(new_dict.keys())\n",
    "torch.save(new_dict, 'marlin_model.bin')"
   ]
  },
  {
   "source": [
    "### Run Inference - modify test.py to remove trainer.train()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = 'test.py'\n",
    "codepath = '.'\n",
    "\n",
    "from azureml.core import Dataset\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "#create input/output datasets\n",
    "def ds_input_path(path):\n",
    "    return Dataset.File.from_files(ds.path(path))\n",
    "def ds_output_path(path):\n",
    "        return OutputFileDatasetConfig(destination=(ds, path))\n",
    "\n",
    "def get_args():\n",
    "    all_params_default = [\n",
    "                    '--data.train_filepath', './train_germ/train.tsv',\n",
    "                    '--data.val_filepath', './val_germ/dev.tsv',\n",
    "                    '--config_path', 'config_germ.yaml',\n",
    "                    '--model.model_path', '< Modify to point to model directory>',\n",
    "                    '--model.model_file', 'marlin_model.bin'\n",
    "    ]\n",
    "    return all_params_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "myenv = Environment(name=\"myenv\")\n",
    "myenv.docker.base_image = 'jonrsleep/elr2:latest'\n",
    "myenv.python.interpreter_path = '/opt/miniconda/envs/elr2/bin/python'\n",
    "myenv.python.user_managed_dependencies = True\n",
    "\n",
    "mpi = MpiConfiguration() \n",
    "mpi.process_count_per_node = 1 #NC SKU has 4 GPU's per node\n",
    "mpi.node_count = 1 #scale to the amount of nodes you'd like\n",
    "\n",
    "config = ScriptRunConfig(source_directory=codepath,\n",
    "                         script=script_name,\n",
    "                         arguments = get_args(),\n",
    "                         compute_target=gpu_compute_target,\n",
    "                         environment=myenv,\n",
    "                         distributed_job_config=mpi)\n",
    "\n",
    "experiment_name = 'marlin_ner_train_plugin_germ_inf'\n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "run = experiment.submit(config)\n",
    "run.tag('nodes', f'{mpi.node_count}')\n",
    "run.tag('exp', 'lr 3e-5 ')\n",
    "print(\"Submitted run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "aagarg"
   }
  ],
  "kernelspec": {
   "name": "python376jvsc74a57bd0bfcc0e8082d67514189d0b0ab8037e090a59025f01a56f76324e6a6466904f5e",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "msauthor": "aagarg"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}