# Using abreviated letter coding for group names. This will be used to parse these arguments when passed from command line.
# Example usage in command-line: --tmod.max_lr 4E-5

# data-module arguments
dmod:
    input_dir : ""
    output_dir : ""

# data-processor args
dproc:
    task: null
    max_seq_len: 128
    no_labels: False
    set_type: null
    tokenizer: "bert-base-uncased"

# model arguments
model:
    model_name: "bert"
    encoder_key: "bert"
    hf_model: "bert-base-uncased"
    model_file: "pytorch_model.bin"
    model_wts_path: null
    model_config_file: "config.json"
    model_config_path: null
    get_latest_ckpt: True
    num_labels: null

# Module interface arguments
module:
    operation: "train"
    trainer_backend: "DDPTrainerBackend"
    fp16: False
    task: null
    trainpath: ""
    valpath: ""
    output_dir: ""
    max_lr : 0.00002 # Maximum learning rate.
    warmup_prop: 0.1
    num_files: -1
    no_labels: False
    log_level: "DEBUG"

# distillation arguments
distill:
    student_layers: "[0-6-11]"
    loss_types: "[logits-attentions]"
    loss_weights: "[1-1]"
    temperature: 1
    width_shrinkage: 0.75

# trainer arguments
trainer:
    # max_train_steps_per_epoch : 50 # Maximum train steps per epoch.
    # max_val_steps_per_epoch : 20 # Maximum validation steps per epoch.
    train_batch_size: 32 # Training global batch size.
    val_batch_size: 16 # Validation global batch size.
    epochs: 3 # Total epochs to run.
    gpu_batch_size_limit : 8 # Max limit for GPU batch size during training.
    clip_grads : True # Enable or disable clipping of gradients.
    use_gpu: True # Enable or disable use of GPU.
    max_grad_norm: 1.0 # Maximum value for gradient norm.
    writers: ['stdout', 'aml', 'tensorboard'] # List of all the writers to use.
    disable_tqdm: True
    log_level: "DEBUG"
    reset_optimizers_schedulers: True

# Checkpointer arguments
ckpt:
    checkpoint: True # Flag indicating whether to checkpoint model.
    delete_existing_checkpoints: False
    period: 1 # Period of epochs at which to checkpoint model.
    save_dir: 'ckpts' # Path to directory where checkpoints are to be stored.
    file_prefix: 'bert_ddp' # Prefix of the checkpoint filename.
    file_ext: 'pt' # File extension for the checkpoint.
    load_dir: ''
    load_filename: ''
    log_level: 'DEBUG'

# Basic-Statistics arguments
stats:
    log_steps: 5
    update_system_stats: False
    log_model_steps: 1000
    exclude_list: 'bias|LayerNorm|layer\\.[3-9]|layer\\.1(?!1)|layer\\.2(?!3)'

# Writers arguments
wrts:
    model_log_level : 'INFO'
    tb_log_dir : 'logs'
    tb_logpath_parent_env : null
    tb_log_multi : False
    tb_log_hist_steps : 20000