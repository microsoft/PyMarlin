{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint hyperdrive = azureml.train.hyperdrive:HyperDriveRun._from_run_dto with exception (azureml-core 1.26.0 (c:\\users\\ashwinsr\\anaconda3\\lib\\site-packages), Requirement.parse('azureml-core~=1.21.0'), {'azureml-telemetry'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception (azureml-core 1.26.0 (c:\\users\\ashwinsr\\anaconda3\\lib\\site-packages), Requirement.parse('azureml-core~=1.21.0'), {'azureml-telemetry'}).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.PipelineRun = azureml.pipeline.core.run:PipelineRun._from_dto with exception (azureml-core 1.26.0 (c:\\users\\ashwinsr\\anaconda3\\lib\\site-packages), Requirement.parse('azureml-core~=1.21.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.ReusedStepRun = azureml.pipeline.core.run:StepRun._from_reused_dto with exception (azureml-core 1.26.0 (c:\\users\\ashwinsr\\anaconda3\\lib\\site-packages), Requirement.parse('azureml-core~=1.21.0')).\n",
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint azureml.StepRun = azureml.pipeline.core.run:StepRun._from_dto with exception (azureml-core 1.26.0 (c:\\users\\ashwinsr\\anaconda3\\lib\\site-packages), Requirement.parse('azureml-core~=1.21.0')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.26.0\n"
     ]
    }
   ],
   "source": [
    "# For automatic reloading of modified libraries\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Regular python libraries\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import json\n",
    "import statistics\n",
    "\n",
    "import torch\n",
    "\n",
    "# AzureML libraries\n",
    "import azureml\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Datastore, ScriptRunConfig\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.core.runconfig import MpiConfiguration, RunConfiguration, DEFAULT_GPU_IMAGE\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have logged in. Now let us find all the subscriptions to which you have access...\n",
      "Interactive authentication successfully completed.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "forced_interactive_auth = InteractiveLoginAuthentication(tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\", force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "To create or access an Azure ML Workspace, you will need to import the AML library and the following information:\n",
    "* A name for your workspace\n",
    "* Your subscription id\n",
    "* The resource group name\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step or create a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:\t\tSubstrateIntelligenceNLR-WS2\n",
      "Location:\teastus\n"
     ]
    }
   ],
   "source": [
    "subscription_id = '<Subscription id>'\n",
    "resource_group = '<resource group>'\n",
    "workspace_name = '<Workspace name>'\n",
    "ws = Workspace(subscription_id, resource_group, workspace_name)\n",
    "ws_details = ws.get_details()\n",
    "print('Name:\\t\\t{}\\nLocation:\\t{}'\n",
    "      .format(ws_details['name'],\n",
    "              ws_details['location']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datastore name: workspaceblobstore\n",
      "Container name: azureml-blobstore-d6fc2475-ad02-44a7-90ff-88a2a91e66b1\n",
      "Datastore type: AzureBlob\n",
      "Workspace name: SubstrateIntelligenceNLR-WS2\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "ds = ws.get_default_datastore()\n",
    "#ds = Datastore.get(ws,'default')\n",
    "#ds = Datastore.get(ws,'workspaceblobstore')\n",
    "print('Datastore name: ' + ds.name, \n",
    "      'Container name: ' + ds.container_name, \n",
    "      'Datastore type: ' + ds.datastore_type, \n",
    "      'Workspace name: ' + ds.workspace.name, sep = '\\n')\n",
    "# ws.get_default_datastore().container_name\n",
    "# ws.datastores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 4, 'targetNodeCount': 4, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 2, 'idleNodeCount': 2, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2021-10-27T03:51:36.173000+00:00', 'errors': None, 'creationTime': '2021-02-11T21:11:09.969817+00:00', 'modifiedTime': '2021-10-22T18:19:47.365070+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 4, 'maxNodeCount': 16, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC24RS_V3'}\n"
     ]
    }
   ],
   "source": [
    "# Create the compute cluster\n",
    "gpu_cluster_name = \"<cluster name>\"\n",
    "\n",
    "# Verify that the cluster doesn't exist already\n",
    "try:\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    if gpu_compute_target.provisioning_state == 'Failed':\n",
    "        gpu_compute_target.delete()\n",
    "        gpu_compute_target.wait_for_completion(show_output=True)\n",
    "        raise ComputeTargetException('failed cluster')\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_priority='lowpriority' , \n",
    "                                                           vm_size='Standard_NC24rs_v3', \n",
    "                                                           min_nodes=0, max_nodes=16)\n",
    "    # ^^^ Change to min_nodes=8 and max_nodes=64 when testing is completed^^^\n",
    "    \n",
    "    # create the cluster\n",
    "    gpu_compute_target = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "    gpu_compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current cluster. \n",
    "print(gpu_compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "\n",
    "run_user_managed = RunConfiguration()\n",
    "run_user_managed.environment.python.user_managed_dependencies = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLUE setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = 'src/train.py'\n",
    "codepath = '.'\n",
    "\n",
    "from azureml.core import Dataset\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "#create input/output datasets\n",
    "def ds_input_path(path):\n",
    "    return Dataset.File.from_files(ds.path(path))\n",
    "def ds_output_path(path):\n",
    "        return OutputFileDatasetConfig(destination=(ds, path))\n",
    "\n",
    "def get_args():\n",
    "    all_params_default = [\n",
    "                    '--config_path', './configs-bert-base/snli.yaml'\n",
    "    ]\n",
    "    return all_params_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted run\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "myenv = Environment(name=\"myenv\")\n",
    "myenv.docker.base_image = 'ashwin2807/pymarlindp:latest'#'pymarlin/pymarlin.cuda11.1:latest'#'jonrsleep/elr2:latest'\n",
    "myenv.python.interpreter_path = '/opt/miniconda/bin/python'\n",
    "myenv.python.user_managed_dependencies = True\n",
    "\n",
    "mpi = MpiConfiguration() \n",
    "mpi.process_count_per_node = 1 #NC SKU has 4 GPU's per node\n",
    "mpi.node_count = 1 #scale to the amount of nodes you'd like\n",
    "\n",
    "config = ScriptRunConfig(source_directory=codepath,\n",
    "                         script=script_name,\n",
    "                         arguments = get_args(),\n",
    "                         compute_target=gpu_compute_target,\n",
    "                         environment=myenv,\n",
    "                         distributed_job_config=mpi)\n",
    "\n",
    "experiment_name = 'dpsgd_snli_parity'\n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "run = experiment.submit(config)\n",
    "run.tag('nodes', f'{mpi.node_count}')\n",
    "#run.tag('sample_rate', '0.004')\n",
    "#run.tag('Noise', '0.4')\n",
    "#run.tag('lr', '2e-4')\n",
    "#run.tag('exp', 'lr 3e-5 ')\n",
    "print(\"Submitted run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distrib eval test\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model checkpoint modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "state_dict = torch.load('marlin_0.bin', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Modify to point to model \n",
    "new_dict = OrderedDict((key.replace('model.',''), value) for key, value in state_dict['module_interface_state'].items() if key.startswith('model.') )\n",
    "#print(new_dict.keys())\n",
    "torch.save(new_dict, 'marlin_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference - modify test.py to remove trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = 'test.py'\n",
    "codepath = '.'\n",
    "\n",
    "from azureml.core import Dataset\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "#create input/output datasets\n",
    "def ds_input_path(path):\n",
    "    return Dataset.File.from_files(ds.path(path))\n",
    "def ds_output_path(path):\n",
    "        return OutputFileDatasetConfig(destination=(ds, path))\n",
    "\n",
    "def get_args():\n",
    "    all_params_default = [\n",
    "                    '--data.train_filepath', './train_germ/train.tsv',\n",
    "                    '--data.val_filepath', './val_germ/dev.tsv',\n",
    "                    '--config_path', 'config_germ.yaml',\n",
    "                    '--model.model_path', '< Modify to point to model directory>',\n",
    "                    '--model.model_file', 'marlin_model.bin'\n",
    "    ]\n",
    "    return all_params_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "myenv = Environment(name=\"myenv\")\n",
    "myenv.docker.base_image = 'jonrsleep/elr2:latest'\n",
    "myenv.python.interpreter_path = '/opt/miniconda/envs/elr2/bin/python'\n",
    "myenv.python.user_managed_dependencies = True\n",
    "\n",
    "mpi = MpiConfiguration() \n",
    "mpi.process_count_per_node = 1 #NC SKU has 4 GPU's per node\n",
    "mpi.node_count = 1 #scale to the amount of nodes you'd like\n",
    "\n",
    "config = ScriptRunConfig(source_directory=codepath,\n",
    "                         script=script_name,\n",
    "                         arguments = get_args(),\n",
    "                         compute_target=gpu_compute_target,\n",
    "                         environment=myenv,\n",
    "                         distributed_job_config=mpi)\n",
    "\n",
    "experiment_name = 'marlin_ner_train_plugin_germ_inf'\n",
    "experiment = Experiment(ws, name=experiment_name)\n",
    "run = experiment.submit(config)\n",
    "run.tag('nodes', f'{mpi.node_count}')\n",
    "run.tag('exp', 'lr 3e-5 ')\n",
    "print(\"Submitted run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "aagarg"
   }
  ],
  "interpreter": {
   "hash": "bfcc0e8082d67514189d0b0ab8037e090a59025f01a56f76324e6a6466904f5e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "msauthor": "aagarg"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
