glue_task: 'qnli'

mi:
    encoder : "bert-base-uncased" #"roberta-base"
    tokenizer: "bert-base-uncased" #"roberta-base"
    num_labels : 2
    max_lr : 0.00002
    s1_key : 'question'
    s2_key : 'sentence'
    max_length : 128
tr:
    max_train_steps_per_epoch : null # Maximum train steps per epoch.
    max_val_steps_per_epoch : null # Maximum validation steps per epoch.
    train_batch_size: 32 # Training global batch size.
    val_batch_size: 64 # Validation global batch size.
    epochs: 3 # Total epochs to run.
    gpu_batch_size_limit : 8 # Max limit for GPU batch size during training.
    disable_tqdm : False
    writers: ['stdout', 'aml', 'tensorboard']
    backend: 'sp'

opacus:
    sample_rate: 0.0001 #0.0004
    batch_size: null
    sample_size: null
    max_grad_norm: 1.0
    noise_multiplier: 0.8
    #alphas: List[float] = DEFAULT_ALPHAS,  Removed since default lies in Privacy engine
    secure_rng: False
    batch_first: True
    target_delta: 0.000006
    target_epsilon: null #Float
    epochs: null #int
    loss_reduction: "mean" #Indicates if the loss reduction (for aggregating the gradients) is a sum or a mean operation. Can take values "sum" or "mean"
    poisson: False

wrt:
    tb_log_dir : 'logs_roberta_base/qnli/from_pretrained'

stat:
    log_steps : 20
dist:
    local_rank : 0
ckp:
    checkpoint : False
    period: 2